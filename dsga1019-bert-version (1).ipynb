{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-04-26T18:10:41.564332Z","iopub.execute_input":"2023-04-26T18:10:41.565038Z","iopub.status.idle":"2023-04-26T18:10:41.575111Z","shell.execute_reply.started":"2023-04-26T18:10:41.564998Z","shell.execute_reply":"2023-04-26T18:10:41.573609Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"/kaggle/input/nlp-getting-started/sample_submission.csv\n/kaggle/input/nlp-getting-started/train.csv\n/kaggle/input/nlp-getting-started/test.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom transformers import AutoModel, AutoTokenizer, get_cosine_schedule_with_warmup, AutoConfig\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Sampler, Dataset, DataLoader\nfrom IPython.display import display\nfrom accelerate import Accelerator\nfrom tqdm import tqdm\nimport random\nimport copy\nimport os\nimport multiprocessing\nfrom sklearn.model_selection import StratifiedKFold\n# import more_itertools\nfrom sklearn.metrics import f1_score\n\nimport string\nfrom bs4 import BeautifulSoup\nimport re","metadata":{"execution":{"iopub.status.busy":"2023-04-26T18:10:41.576864Z","iopub.execute_input":"2023-04-26T18:10:41.577732Z","iopub.status.idle":"2023-04-26T18:10:41.585391Z","shell.execute_reply.started":"2023-04-26T18:10:41.577696Z","shell.execute_reply":"2023-04-26T18:10:41.584152Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Data preprocess","metadata":{}},{"cell_type":"code","source":"train_raw = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\nsubmission = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-04-26T18:10:41.587149Z","iopub.execute_input":"2023-04-26T18:10:41.587889Z","iopub.status.idle":"2023-04-26T18:10:41.633924Z","shell.execute_reply.started":"2023-04-26T18:10:41.587825Z","shell.execute_reply":"2023-04-26T18:10:41.632818Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"train_no_duplicates = train_raw.drop_duplicates('text')","metadata":{"execution":{"iopub.status.busy":"2023-04-26T18:10:41.685867Z","iopub.execute_input":"2023-04-26T18:10:41.686550Z","iopub.status.idle":"2023-04-26T18:10:41.697622Z","shell.execute_reply.started":"2023-04-26T18:10:41.686514Z","shell.execute_reply":"2023-04-26T18:10:41.696586Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# remove contradictory\nduplicates_df = train_raw[train_raw.text.duplicated(keep=False)].sort_values('text')\ncontradictory_tweets = set()\nfor tweet in list(duplicates_df.text):\n    if len(set(duplicates_df[duplicates_df['text'] == tweet].target)) > 1:\n        contradictory_tweets.add(tweet)\n\ncontradictory_tweets = list(contradictory_tweets)\nprint(len(contradictory_tweets))","metadata":{"execution":{"iopub.status.busy":"2023-04-26T18:10:41.699163Z","iopub.execute_input":"2023-04-26T18:10:41.699794Z","iopub.status.idle":"2023-04-26T18:10:41.815665Z","shell.execute_reply.started":"2023-04-26T18:10:41.699757Z","shell.execute_reply":"2023-04-26T18:10:41.814535Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"18\n","output_type":"stream"}]},{"cell_type":"code","source":"filtered_df = train_no_duplicates[~train_no_duplicates['text'].isin(contradictory_tweets)]\n# filtered_df = filtered_df.drop(columns=['id'])","metadata":{"execution":{"iopub.status.busy":"2023-04-26T18:10:42.039334Z","iopub.execute_input":"2023-04-26T18:10:42.039803Z","iopub.status.idle":"2023-04-26T18:10:42.049840Z","shell.execute_reply.started":"2023-04-26T18:10:42.039759Z","shell.execute_reply":"2023-04-26T18:10:42.048603Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"filtered_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-04-26T18:10:45.182953Z","iopub.execute_input":"2023-04-26T18:10:45.183896Z","iopub.status.idle":"2023-04-26T18:10:45.200226Z","shell.execute_reply.started":"2023-04-26T18:10:45.183833Z","shell.execute_reply":"2023-04-26T18:10:45.199116Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"   id keyword location                                               text  \\\n0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n\n   target  \n0       1  \n1       1  \n2       1  \n3       1  \n4       1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Our Deeds are the Reason of this #earthquake M...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Forest fire near La Ronge Sask. Canada</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>All residents asked to 'shelter in place' are ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>13,000 people receive #wildfires evacuation or...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"def combine_columns(row):\n    values = [f\"{col}: {str(row[col])}\" for col in row.index[1:-2] if pd.notnull(row[col])]\n    return ' '.join(values)","metadata":{"execution":{"iopub.status.busy":"2023-04-26T18:10:47.640216Z","iopub.execute_input":"2023-04-26T18:10:47.641211Z","iopub.status.idle":"2023-04-26T18:10:47.646796Z","shell.execute_reply.started":"2023-04-26T18:10:47.641174Z","shell.execute_reply":"2023-04-26T18:10:47.645687Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# Combine the three columns into a single column\nfiltered_df['combined'] = filtered_df.apply(combine_columns, axis=1)\nfiltered_df['final_text'] =  filtered_df['text']+' '+ filtered_df['combined']\nfiltered_df","metadata":{"execution":{"iopub.status.busy":"2023-04-26T18:10:49.510662Z","iopub.execute_input":"2023-04-26T18:10:49.511585Z","iopub.status.idle":"2023-04-26T18:10:49.724585Z","shell.execute_reply.started":"2023-04-26T18:10:49.511533Z","shell.execute_reply":"2023-04-26T18:10:49.723439Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  \n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  This is separate from the ipykernel package so we can avoid doing imports until\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"         id keyword location  \\\n0         1     NaN      NaN   \n1         4     NaN      NaN   \n2         5     NaN      NaN   \n3         6     NaN      NaN   \n4         7     NaN      NaN   \n...     ...     ...      ...   \n7604  10863     NaN      NaN   \n7605  10864     NaN      NaN   \n7606  10866     NaN      NaN   \n7608  10869     NaN      NaN   \n7612  10873     NaN      NaN   \n\n                                                   text  target combined  \\\n0     Our Deeds are the Reason of this #earthquake M...       1            \n1                Forest fire near La Ronge Sask. Canada       1            \n2     All residents asked to 'shelter in place' are ...       1            \n3     13,000 people receive #wildfires evacuation or...       1            \n4     Just got sent this photo from Ruby #Alaska as ...       1            \n...                                                 ...     ...      ...   \n7604  #WorldNews Fallen powerlines on G:link tram: U...       1            \n7605  on the flip side I'm at Walmart and there is a...       1            \n7606  Suicide bomber kills 15 in Saudi security site...       1            \n7608  Two giant cranes holding a bridge collapse int...       1            \n7612  The Latest: More Homes Razed by Northern Calif...       1            \n\n                                             final_text  \n0     Our Deeds are the Reason of this #earthquake M...  \n1               Forest fire near La Ronge Sask. Canada   \n2     All residents asked to 'shelter in place' are ...  \n3     13,000 people receive #wildfires evacuation or...  \n4     Just got sent this photo from Ruby #Alaska as ...  \n...                                                 ...  \n7604  #WorldNews Fallen powerlines on G:link tram: U...  \n7605  on the flip side I'm at Walmart and there is a...  \n7606  Suicide bomber kills 15 in Saudi security site...  \n7608  Two giant cranes holding a bridge collapse int...  \n7612  The Latest: More Homes Razed by Northern Calif...  \n\n[7485 rows x 7 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n      <th>combined</th>\n      <th>final_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Our Deeds are the Reason of this #earthquake M...</td>\n      <td>1</td>\n      <td></td>\n      <td>Our Deeds are the Reason of this #earthquake M...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Forest fire near La Ronge Sask. Canada</td>\n      <td>1</td>\n      <td></td>\n      <td>Forest fire near La Ronge Sask. Canada</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>All residents asked to 'shelter in place' are ...</td>\n      <td>1</td>\n      <td></td>\n      <td>All residents asked to 'shelter in place' are ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>13,000 people receive #wildfires evacuation or...</td>\n      <td>1</td>\n      <td></td>\n      <td>13,000 people receive #wildfires evacuation or...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n      <td>1</td>\n      <td></td>\n      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>7604</th>\n      <td>10863</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>#WorldNews Fallen powerlines on G:link tram: U...</td>\n      <td>1</td>\n      <td></td>\n      <td>#WorldNews Fallen powerlines on G:link tram: U...</td>\n    </tr>\n    <tr>\n      <th>7605</th>\n      <td>10864</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>on the flip side I'm at Walmart and there is a...</td>\n      <td>1</td>\n      <td></td>\n      <td>on the flip side I'm at Walmart and there is a...</td>\n    </tr>\n    <tr>\n      <th>7606</th>\n      <td>10866</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Suicide bomber kills 15 in Saudi security site...</td>\n      <td>1</td>\n      <td></td>\n      <td>Suicide bomber kills 15 in Saudi security site...</td>\n    </tr>\n    <tr>\n      <th>7608</th>\n      <td>10869</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Two giant cranes holding a bridge collapse int...</td>\n      <td>1</td>\n      <td></td>\n      <td>Two giant cranes holding a bridge collapse int...</td>\n    </tr>\n    <tr>\n      <th>7612</th>\n      <td>10873</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>The Latest: More Homes Razed by Northern Calif...</td>\n      <td>1</td>\n      <td></td>\n      <td>The Latest: More Homes Razed by Northern Calif...</td>\n    </tr>\n  </tbody>\n</table>\n<p>7485 rows × 7 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"#test set\nfiltered_test = test.copy()\nfiltered_test['combined'] = filtered_test.apply(combine_columns, axis=1)\nfiltered_test['final_text'] =  filtered_test['text']+' '+ filtered_test['combined']\nfiltered_test","metadata":{"execution":{"iopub.status.busy":"2023-04-26T18:10:56.329970Z","iopub.execute_input":"2023-04-26T18:10:56.330948Z","iopub.status.idle":"2023-04-26T18:10:56.414002Z","shell.execute_reply.started":"2023-04-26T18:10:56.330909Z","shell.execute_reply":"2023-04-26T18:10:56.413050Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"         id keyword location  \\\n0         0     NaN      NaN   \n1         2     NaN      NaN   \n2         3     NaN      NaN   \n3         9     NaN      NaN   \n4        11     NaN      NaN   \n...     ...     ...      ...   \n3258  10861     NaN      NaN   \n3259  10865     NaN      NaN   \n3260  10868     NaN      NaN   \n3261  10874     NaN      NaN   \n3262  10875     NaN      NaN   \n\n                                                   text combined  \\\n0                    Just happened a terrible car crash            \n1     Heard about #earthquake is different cities, s...            \n2     there is a forest fire at spot pond, geese are...            \n3              Apocalypse lighting. #Spokane #wildfires            \n4         Typhoon Soudelor kills 28 in China and Taiwan            \n...                                                 ...      ...   \n3258  EARTHQUAKE SAFETY LOS ANGELES ÛÒ SAFETY FASTE...            \n3259  Storm in RI worse than last hurricane. My city...            \n3260  Green Line derailment in Chicago http://t.co/U...            \n3261  MEG issues Hazardous Weather Outlook (HWO) htt...            \n3262  #CityofCalgary has activated its Municipal Eme...            \n\n                                             final_text  \n0                   Just happened a terrible car crash   \n1     Heard about #earthquake is different cities, s...  \n2     there is a forest fire at spot pond, geese are...  \n3             Apocalypse lighting. #Spokane #wildfires   \n4        Typhoon Soudelor kills 28 in China and Taiwan   \n...                                                 ...  \n3258  EARTHQUAKE SAFETY LOS ANGELES ÛÒ SAFETY FASTE...  \n3259  Storm in RI worse than last hurricane. My city...  \n3260  Green Line derailment in Chicago http://t.co/U...  \n3261  MEG issues Hazardous Weather Outlook (HWO) htt...  \n3262  #CityofCalgary has activated its Municipal Eme...  \n\n[3263 rows x 6 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>combined</th>\n      <th>final_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Just happened a terrible car crash</td>\n      <td></td>\n      <td>Just happened a terrible car crash</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Heard about #earthquake is different cities, s...</td>\n      <td></td>\n      <td>Heard about #earthquake is different cities, s...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>there is a forest fire at spot pond, geese are...</td>\n      <td></td>\n      <td>there is a forest fire at spot pond, geese are...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>9</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Apocalypse lighting. #Spokane #wildfires</td>\n      <td></td>\n      <td>Apocalypse lighting. #Spokane #wildfires</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>11</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n      <td></td>\n      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3258</th>\n      <td>10861</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>EARTHQUAKE SAFETY LOS ANGELES ÛÒ SAFETY FASTE...</td>\n      <td></td>\n      <td>EARTHQUAKE SAFETY LOS ANGELES ÛÒ SAFETY FASTE...</td>\n    </tr>\n    <tr>\n      <th>3259</th>\n      <td>10865</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Storm in RI worse than last hurricane. My city...</td>\n      <td></td>\n      <td>Storm in RI worse than last hurricane. My city...</td>\n    </tr>\n    <tr>\n      <th>3260</th>\n      <td>10868</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Green Line derailment in Chicago http://t.co/U...</td>\n      <td></td>\n      <td>Green Line derailment in Chicago http://t.co/U...</td>\n    </tr>\n    <tr>\n      <th>3261</th>\n      <td>10874</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>MEG issues Hazardous Weather Outlook (HWO) htt...</td>\n      <td></td>\n      <td>MEG issues Hazardous Weather Outlook (HWO) htt...</td>\n    </tr>\n    <tr>\n      <th>3262</th>\n      <td>10875</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>#CityofCalgary has activated its Municipal Eme...</td>\n      <td></td>\n      <td>#CityofCalgary has activated its Municipal Eme...</td>\n    </tr>\n  </tbody>\n</table>\n<p>3263 rows × 6 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# DataLoader","metadata":{}},{"cell_type":"code","source":"device = \"cuda:0\"\nmodel_checkpoint = \"bert-large-uncased\"","metadata":{"execution":{"iopub.status.busy":"2023-04-26T18:11:00.023890Z","iopub.execute_input":"2023-04-26T18:11:00.024257Z","iopub.status.idle":"2023-04-26T18:11:00.029689Z","shell.execute_reply.started":"2023-04-26T18:11:00.024225Z","shell.execute_reply":"2023-04-26T18:11:00.028341Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)","metadata":{"execution":{"iopub.status.busy":"2023-04-26T18:11:03.154803Z","iopub.execute_input":"2023-04-26T18:11:03.155292Z","iopub.status.idle":"2023-04-26T18:11:06.967583Z","shell.execute_reply.started":"2023-04-26T18:11:03.155257Z","shell.execute_reply":"2023-04-26T18:11:06.966573Z"},"trusted":true},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8ea72b8a6d043d5b750205c9862b634"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"49063c90034d491dbee7afc724f7b65f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d8becdcb5b747739d1b4ee3ece825f8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b00f22ae69194cc5891c70913c6dfe9f"}},"metadata":{}}]},{"cell_type":"code","source":"class MyDataset(Dataset):\n    def __init__(self, dataframe, text_column, tokenizer, target='target', max_length = 256):\n#         text = dataframe[text_column].values.tolist()\n#         tokenized = tokenizer(text, padding=\"max_length\", truncation=True, max_length=max_length, return_tensors='pt')\n        self.data = []\n        \n        for index, row in tqdm(dataframe.iterrows(), total=len(dataframe), ncols=70):\n#             self.data.append((torch.tensor(row['id']).to(device) ,(tokenized['input_ids'][0].to(device), tokenized['attention_mask'][0].to(device))))\n            text = row[text_column]\n            tokenized = tokenizer(text, padding=\"max_length\", truncation=True, max_length=256, return_tensors='pt')\n            self.data.append(((tokenized['input_ids'][0].to(device), tokenized['attention_mask'][0].to(device)), torch.tensor(row[target]).to(device)))\n    \n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        return self.data[idx]\n    \n    def train_valid_split(self, train_fraction=.8, shuffle=True):\n        num_train_examples = int(len(self) * train_fraction)\n        train_dataset = copy.deepcopy(self)\n        \n        if shuffle:\n            random.shuffle(train_dataset.data)\n        \n        valid_dataset = copy.deepcopy(train_dataset)\n        train_dataset.data = train_dataset.data[:num_train_examples]\n        valid_dataset.data = valid_dataset.data[num_train_examples:]\n        \n        return train_dataset, valid_dataset","metadata":{"execution":{"iopub.status.busy":"2023-04-26T18:11:15.854189Z","iopub.execute_input":"2023-04-26T18:11:15.854874Z","iopub.status.idle":"2023-04-26T18:11:15.866854Z","shell.execute_reply.started":"2023-04-26T18:11:15.854836Z","shell.execute_reply":"2023-04-26T18:11:15.865798Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# Create an instance of your custom Dataset class\ndataset = MyDataset(filtered_df, 'final_text', tokenizer)\ntrain_dataset, valid_dataset = dataset.train_valid_split()","metadata":{"execution":{"iopub.status.busy":"2023-04-26T18:11:21.627871Z","iopub.execute_input":"2023-04-26T18:11:21.628290Z","iopub.status.idle":"2023-04-26T18:11:33.971018Z","shell.execute_reply.started":"2023-04-26T18:11:21.628251Z","shell.execute_reply":"2023-04-26T18:11:33.969956Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"100%|████████████████████████████| 7485/7485 [00:09<00:00, 821.18it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"class MyDatasetTest(Dataset):\n    def __init__(self, dataframe, text_column, tokenizer, max_length = 256):\n#         text = dataframe[text_column].values.tolist()\n#         tokenized = tokenizer(text, padding=\"max_length\", truncation=True, max_length=max_length, return_tensors='pt')\n        self.data = []\n        \n        for index, row in tqdm(dataframe.iterrows(), total=len(dataframe), ncols=70):\n#             self.data.append((torch.tensor(row['id']).to(device) ,(tokenized['input_ids'][0].to(device), tokenized['attention_mask'][0].to(device))))\n            text = row[text_column]\n            tokenized = tokenizer(text, padding=\"max_length\", truncation=True, max_length=256, return_tensors='pt')\n#             self.data.append(((tokenized['input_ids'][0].to(device), tokenized['attention_mask'][0].to(device)), torch.tensor(row[target]).to(device)))\n            self.data.append((torch.tensor(row['id']).to(device) ,(tokenized['input_ids'][0].to(device), tokenized['attention_mask'][0].to(device))))\n    \n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        return self.data[idx]","metadata":{"execution":{"iopub.status.busy":"2023-04-26T18:15:20.850274Z","iopub.execute_input":"2023-04-26T18:15:20.850676Z","iopub.status.idle":"2023-04-26T18:15:20.859734Z","shell.execute_reply.started":"2023-04-26T18:15:20.850634Z","shell.execute_reply":"2023-04-26T18:15:20.858530Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"dataset_test = MyDatasetTest(filtered_test, 'final_text', tokenizer)","metadata":{"execution":{"iopub.status.busy":"2023-04-26T18:15:26.588541Z","iopub.execute_input":"2023-04-26T18:15:26.589000Z","iopub.status.idle":"2023-04-26T18:15:28.517258Z","shell.execute_reply.started":"2023-04-26T18:15:26.588959Z","shell.execute_reply":"2023-04-26T18:15:28.516093Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stderr","text":"100%|███████████████████████████| 3263/3263 [00:01<00:00, 1701.45it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"# tokenizer.decode(train_dataset[100][1][0])","metadata":{"execution":{"iopub.status.busy":"2023-04-26T02:00:36.828628Z","iopub.execute_input":"2023-04-26T02:00:36.828943Z","iopub.status.idle":"2023-04-26T02:00:36.834127Z","shell.execute_reply.started":"2023-04-26T02:00:36.828915Z","shell.execute_reply":"2023-04-26T02:00:36.832887Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# Create a DataLoader\nbatch_size = 8\ntrain_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nvalid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2023-04-26T18:15:33.907863Z","iopub.execute_input":"2023-04-26T18:15:33.908252Z","iopub.status.idle":"2023-04-26T18:15:33.914351Z","shell.execute_reply.started":"2023-04-26T18:15:33.908217Z","shell.execute_reply":"2023-04-26T18:15:33.913071Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"# Set up BERT","metadata":{}},{"cell_type":"code","source":"class FinetuneClassifier(nn.Module):\n    def __init__(self, model=model_checkpoint, classes=2, head_dropout=0.2):\n        super().__init__()\n        \n        self.model = AutoModel.from_pretrained(model)\n        \n        self.project = torch.nn.Sequential(\n            torch.nn.Dropout(head_dropout),\n            torch.nn.Linear(1024, 1024),\n            torch.nn.Dropout(head_dropout),\n            torch.nn.Linear(1024,classes) # projection\n        )\n\n    def forward(self, input_ids, attention_mask=None):\n        \n        res = self.model.forward(input_ids=input_ids, attention_mask=attention_mask, return_dict=False)\n        res = res[0]\n        res = res[:,0,:] # encoding for <s> token\n        res = self.project(res)\n        return res\n    \n    def parameters_num(self):\n        return sum(p.numel() for p in self.parameters())","metadata":{"execution":{"iopub.status.busy":"2023-04-26T18:15:36.698718Z","iopub.execute_input":"2023-04-26T18:15:36.699846Z","iopub.status.idle":"2023-04-26T18:15:36.708413Z","shell.execute_reply.started":"2023-04-26T18:15:36.699799Z","shell.execute_reply":"2023-04-26T18:15:36.707249Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"model = FinetuneClassifier(head_dropout=.2)","metadata":{"execution":{"iopub.status.busy":"2023-04-26T18:15:42.215526Z","iopub.execute_input":"2023-04-26T18:15:42.216328Z","iopub.status.idle":"2023-04-26T18:16:31.087994Z","shell.execute_reply.started":"2023-04-26T18:15:42.216289Z","shell.execute_reply":"2023-04-26T18:16:31.086945Z"},"trusted":true},"execution_count":27,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/1.34G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f09ad8b57bb415cafe587503828ac0f"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"}]},{"cell_type":"code","source":"model.forward(torch.tensor([1,2,33,2])[None])","metadata":{"execution":{"iopub.status.busy":"2023-04-26T03:06:36.822280Z","iopub.execute_input":"2023-04-26T03:06:36.822673Z","iopub.status.idle":"2023-04-26T03:06:37.192675Z","shell.execute_reply.started":"2023-04-26T03:06:36.822632Z","shell.execute_reply":"2023-04-26T03:06:37.191611Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"tensor([[-0.3648, -0.0454]], grad_fn=<AddmmBackward0>)"},"metadata":{}}]},{"cell_type":"code","source":"model.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-04-26T18:28:35.514409Z","iopub.execute_input":"2023-04-26T18:28:35.515153Z","iopub.status.idle":"2023-04-26T18:28:35.912146Z","shell.execute_reply.started":"2023-04-26T18:28:35.515108Z","shell.execute_reply":"2023-04-26T18:28:35.911115Z"},"trusted":true},"execution_count":28,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"FinetuneClassifier(\n  (model): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 1024, padding_idx=0)\n      (position_embeddings): Embedding(512, 1024)\n      (token_type_embeddings): Embedding(2, 1024)\n      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (1): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (2): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (3): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (4): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (5): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (6): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (7): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (8): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (9): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (10): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (11): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (12): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (13): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (14): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (15): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (16): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (17): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (18): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (19): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (20): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (21): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (22): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (23): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (project): Sequential(\n    (0): Dropout(p=0.2, inplace=False)\n    (1): Linear(in_features=1024, out_features=1024, bias=True)\n    (2): Dropout(p=0.2, inplace=False)\n    (3): Linear(in_features=1024, out_features=2, bias=True)\n  )\n)"},"metadata":{}}]},{"cell_type":"markdown","source":"# Finetuning","metadata":{"execution":{"iopub.status.busy":"2023-04-26T00:27:08.146247Z","iopub.status.idle":"2023-04-26T00:27:08.146868Z","shell.execute_reply.started":"2023-04-26T00:27:08.146557Z","shell.execute_reply":"2023-04-26T00:27:08.146588Z"}}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score","metadata":{"execution":{"iopub.status.busy":"2023-04-26T18:28:42.514416Z","iopub.execute_input":"2023-04-26T18:28:42.515200Z","iopub.status.idle":"2023-04-26T18:28:42.520306Z","shell.execute_reply.started":"2023-04-26T18:28:42.515161Z","shell.execute_reply":"2023-04-26T18:28:42.519009Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"a = next(iter(train_dataloader))\na","metadata":{"execution":{"iopub.status.busy":"2023-04-26T03:07:30.895475Z","iopub.execute_input":"2023-04-26T03:07:30.896250Z","iopub.status.idle":"2023-04-26T03:07:30.909666Z","shell.execute_reply.started":"2023-04-26T03:07:30.896209Z","shell.execute_reply":"2023-04-26T03:07:30.908543Z"},"trusted":true},"execution_count":28,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"[[tensor([[  101, 13215,  1999,  ...,     0,     0,     0],\n          [  101,  1030,  2522,  ...,     0,     0,     0],\n          [  101,  1030,  1051,  ...,     0,     0,     0],\n          ...,\n          [  101,  1030,  3915,  ...,     0,     0,     0],\n          [  101,  1030, 26146,  ...,     0,     0,     0],\n          [  101,  2256, 13044,  ...,     0,     0,     0]], device='cuda:0'),\n  tensor([[1, 1, 1,  ..., 0, 0, 0],\n          [1, 1, 1,  ..., 0, 0, 0],\n          [1, 1, 1,  ..., 0, 0, 0],\n          ...,\n          [1, 1, 1,  ..., 0, 0, 0],\n          [1, 1, 1,  ..., 0, 0, 0],\n          [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')],\n tensor([0, 0, 1, 0, 0, 1, 0, 0], device='cuda:0')]"},"metadata":{}}]},{"cell_type":"code","source":"def train(model, \n          train_dataloader, \n          valid_dataloader, \n          steps, \n          optimizer,\n          blind_steps=None,\n          loss_fn=torch.nn.BCELoss(),\n          main_metric=('f1', f1_score), \n          additional_metrics=[],\n          filepath='model_sd.pt',\n          load_best=True,\n          scheduler=None,\n          losses_dict=None):\n    \n    if blind_steps == None:\n        blind_steps = len(train_dataloader) // 4\n    \n    def evaluate():  # the first score returned is the main\n        model.eval()\n        \n        y_trues = []\n        y_hats = []\n        \n        loss = 0\n        k = 0\n        \n        with torch.no_grad():\n            for batch in valid_dataloader:\n                \n                (ids, mask), y_true = batch\n                ids, mask = ids.to(device), mask.to(device)\n                y_true = y_true.to(device)\n                hots = torch.nn.functional.one_hot(y_true, 2).to(device, torch.float)\n                y_hat = torch.softmax(model.forward(input_ids=ids, attention_mask=mask),dim=-1)\n\n                loss += float(loss_fn(y_hat, hots))\n                k += 1\n                \n                for i in range(y_true.shape[0]):\n                    y_trues.append(int(y_true[i]))\n                    y_hats.append(1 if y_hat[i][0] < y_hat[i][1] else 0)\n        \n        scores = [(main_metric[0], main_metric[1](y_trues, y_hats))]\n        \n        for metric in additional_metrics:\n            scores.append((metric[0], metric[1](y_trues, y_hats)))        \n        \n        model.train()\n        return scores + [('valid_loss', loss/k)]\n    \n    \n    def render_scores(scores, step, best=None):\n        print('{:05d} steps'.format(step), end=' ')\n        \n        for score in scores:\n            print(\"| {}: {:.3f}\".format(*score), end=' ')\n            \n        if best != None:\n            print('| best_score: {:.3f}'.format(best))\n            \n    \n    # initial scores\n    scores = evaluate()\n    render_scores(scores, 0)\n    best_score = scores[0][1]\n    torch.save(model.state_dict(), filepath)\n    \n    # logs\n    if losses_dict != None:\n        losses_dict['train_loss'] = []\n        losses_dict['valid_loss'] = []\n        losses_dict[main_metric[0]] = []\n    \n    epoch_loss = 0\n    k = 0\n    \n    train_iter = iter(train_dataloader)\n    model.train()\n    \n    for step in tqdm(range(steps)):\n        \n        # retrieving a batch\n        try:\n            batch = next(train_iter)\n        except:\n            train_iter = iter(train_dataloader)\n            batch = next(train_iter)\n\n        (ids, mask), y_true = batch\n        ids, mask = ids.to(device), mask.to(device)\n        y_true = y_true.to(device)\n\n        # prediction\n        y_hat = torch.softmax(model.forward(input_ids=ids, attention_mask=mask),dim=-1)\n        hots = torch.nn.functional.one_hot(y_true, 2).to(device, torch.float)\n        loss = loss_fn(y_hat, hots).to(device)\n        \n        # backprop\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        if scheduler != None:\n            scheduler.step()\n            \n        epoch_loss += float(loss)\n        k+=1\n        \n        # evaluation\n        if (step + 1) % blind_steps == 0:\n            scores = evaluate() + [('train_loss', epoch_loss/k)]\n            \n            if losses_dict != None:\n                losses_dict['valid_loss'].append(float(scores[-2][1]))\n                losses_dict['train_loss'].append(float(scores[-1][1]))\n                losses_dict[main_metric[0]].append(float(scores[0][1]))\n            \n            if scores[0][1] > best_score:\n                best_score = scores[0][1]\n                torch.save(model.state_dict(), filepath)\n                \n            render_scores(scores, step + 1, best=best_score)\n            epoch_loss = 0\n            k = 0\n                \n    if load_best:\n        model.load_state_dict(torch.load(filepath))","metadata":{"execution":{"iopub.status.busy":"2023-04-26T18:28:45.952859Z","iopub.execute_input":"2023-04-26T18:28:45.953461Z","iopub.status.idle":"2023-04-26T18:28:45.986507Z","shell.execute_reply.started":"2023-04-26T18:28:45.953415Z","shell.execute_reply":"2023-04-26T18:28:45.985304Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"optimizer = torch.optim.Adam(model.parameters(), lr=5e-6, weight_decay=2e-3)\nscheduler = torch.optim.lr_scheduler.LinearLR(optimizer, total_iters=500) \nlogs_dict = {}","metadata":{"execution":{"iopub.status.busy":"2023-04-26T03:08:36.328539Z","iopub.execute_input":"2023-04-26T03:08:36.329309Z","iopub.status.idle":"2023-04-26T03:08:36.336956Z","shell.execute_reply.started":"2023-04-26T03:08:36.329269Z","shell.execute_reply":"2023-04-26T03:08:36.335821Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"train(\n      model, \n      train_dataloader, \n      valid_dataloader, \n      2000, \n      optimizer, \n      blind_steps=100, \n      additional_metrics=[('precision', precision_score), ('recall', recall_score),('accuracy', accuracy_score)],\n      losses_dict=logs_dict,\n      scheduler=scheduler\n     )","metadata":{"execution":{"iopub.status.busy":"2023-04-26T03:08:38.907466Z","iopub.execute_input":"2023-04-26T03:08:38.908079Z","iopub.status.idle":"2023-04-26T03:48:37.549948Z","shell.execute_reply.started":"2023-04-26T03:08:38.908041Z","shell.execute_reply":"2023-04-26T03:48:37.548935Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"00000 steps | f1: 0.590 | precision: 0.420 | recall: 0.989 | accuracy: 0.426 | valid_loss: 0.711 ","output_type":"stream"},{"name":"stderr","text":"  5%|▌         | 100/2000 [01:59<7:39:25, 14.51s/it]","output_type":"stream"},{"name":"stdout","text":"00100 steps | f1: 0.646 | precision: 0.788 | recall: 0.547 | accuracy: 0.749 | valid_loss: 0.573 | train_loss: 0.641 | best_score: 0.646\n","output_type":"stream"},{"name":"stderr","text":" 10%|█         | 200/2000 [03:58<7:14:18, 14.48s/it]","output_type":"stream"},{"name":"stdout","text":"00200 steps | f1: 0.679 | precision: 0.811 | recall: 0.584 | accuracy: 0.770 | valid_loss: 0.517 | train_loss: 0.542 | best_score: 0.679\n","output_type":"stream"},{"name":"stderr","text":" 15%|█▌        | 300/2000 [05:57<6:49:24, 14.45s/it]","output_type":"stream"},{"name":"stdout","text":"00300 steps | f1: 0.753 | precision: 0.706 | recall: 0.806 | accuracy: 0.779 | valid_loss: 0.464 | train_loss: 0.494 | best_score: 0.753\n","output_type":"stream"},{"name":"stderr","text":" 20%|██        | 400/2000 [07:56<6:23:26, 14.38s/it]","output_type":"stream"},{"name":"stdout","text":"00400 steps | f1: 0.771 | precision: 0.828 | recall: 0.722 | accuracy: 0.821 | valid_loss: 0.425 | train_loss: 0.449 | best_score: 0.771\n","output_type":"stream"},{"name":"stderr","text":" 25%|██▌       | 500/2000 [09:55<5:59:22, 14.38s/it]","output_type":"stream"},{"name":"stdout","text":"00500 steps | f1: 0.783 | precision: 0.763 | recall: 0.805 | accuracy: 0.814 | valid_loss: 0.430 | train_loss: 0.444 | best_score: 0.783\n","output_type":"stream"},{"name":"stderr","text":" 30%|███       | 600/2000 [11:51<5:18:47, 13.66s/it]","output_type":"stream"},{"name":"stdout","text":"00600 steps | f1: 0.696 | precision: 0.903 | recall: 0.566 | accuracy: 0.794 | valid_loss: 0.477 | train_loss: 0.429 | best_score: 0.783\n","output_type":"stream"},{"name":"stderr","text":" 35%|███▌      | 700/2000 [13:50<5:11:06, 14.36s/it]","output_type":"stream"},{"name":"stdout","text":"00700 steps | f1: 0.794 | precision: 0.822 | recall: 0.768 | accuracy: 0.834 | valid_loss: 0.415 | train_loss: 0.398 | best_score: 0.794\n","output_type":"stream"},{"name":"stderr","text":" 40%|████      | 800/2000 [15:46<4:33:14, 13.66s/it]","output_type":"stream"},{"name":"stdout","text":"00800 steps | f1: 0.786 | precision: 0.838 | recall: 0.739 | accuracy: 0.832 | valid_loss: 0.403 | train_loss: 0.399 | best_score: 0.794\n","output_type":"stream"},{"name":"stderr","text":" 45%|████▌     | 900/2000 [17:43<4:10:31, 13.67s/it]","output_type":"stream"},{"name":"stdout","text":"00900 steps | f1: 0.775 | precision: 0.877 | recall: 0.694 | accuracy: 0.832 | valid_loss: 0.397 | train_loss: 0.390 | best_score: 0.794\n","output_type":"stream"},{"name":"stderr","text":" 50%|█████     | 1000/2000 [19:39<3:47:42, 13.66s/it]","output_type":"stream"},{"name":"stdout","text":"01000 steps | f1: 0.788 | precision: 0.853 | recall: 0.733 | accuracy: 0.836 | valid_loss: 0.395 | train_loss: 0.364 | best_score: 0.794\n","output_type":"stream"},{"name":"stderr","text":" 55%|█████▌    | 1100/2000 [21:36<3:24:54, 13.66s/it]","output_type":"stream"},{"name":"stdout","text":"01100 steps | f1: 0.790 | precision: 0.762 | recall: 0.819 | accuracy: 0.818 | valid_loss: 0.461 | train_loss: 0.337 | best_score: 0.794\n","output_type":"stream"},{"name":"stderr","text":" 60%|██████    | 1200/2000 [23:32<3:02:12, 13.67s/it]","output_type":"stream"},{"name":"stdout","text":"01200 steps | f1: 0.792 | precision: 0.818 | recall: 0.768 | accuracy: 0.832 | valid_loss: 0.401 | train_loss: 0.361 | best_score: 0.794\n","output_type":"stream"},{"name":"stderr","text":" 65%|██████▌   | 1300/2000 [25:31<2:47:49, 14.39s/it]","output_type":"stream"},{"name":"stdout","text":"01300 steps | f1: 0.799 | precision: 0.825 | recall: 0.774 | accuracy: 0.837 | valid_loss: 0.396 | train_loss: 0.410 | best_score: 0.799\n","output_type":"stream"},{"name":"stderr","text":" 70%|███████   | 1400/2000 [27:30<2:23:54, 14.39s/it]","output_type":"stream"},{"name":"stdout","text":"01400 steps | f1: 0.806 | precision: 0.823 | recall: 0.789 | accuracy: 0.841 | valid_loss: 0.415 | train_loss: 0.350 | best_score: 0.806\n","output_type":"stream"},{"name":"stderr","text":" 75%|███████▌  | 1500/2000 [29:26<1:53:27, 13.62s/it]","output_type":"stream"},{"name":"stdout","text":"01500 steps | f1: 0.800 | precision: 0.768 | recall: 0.835 | accuracy: 0.826 | valid_loss: 0.447 | train_loss: 0.358 | best_score: 0.806\n","output_type":"stream"},{"name":"stderr","text":" 80%|████████  | 1600/2000 [31:23<1:31:03, 13.66s/it]","output_type":"stream"},{"name":"stdout","text":"01600 steps | f1: 0.802 | precision: 0.774 | recall: 0.832 | accuracy: 0.828 | valid_loss: 0.453 | train_loss: 0.318 | best_score: 0.806\n","output_type":"stream"},{"name":"stderr","text":" 85%|████████▌ | 1700/2000 [33:19<1:08:18, 13.66s/it]","output_type":"stream"},{"name":"stdout","text":"01700 steps | f1: 0.798 | precision: 0.839 | recall: 0.760 | accuracy: 0.839 | valid_loss: 0.421 | train_loss: 0.305 | best_score: 0.806\n","output_type":"stream"},{"name":"stderr","text":" 90%|█████████ | 1800/2000 [35:16<45:32, 13.66s/it]  ","output_type":"stream"},{"name":"stdout","text":"01800 steps | f1: 0.796 | precision: 0.819 | recall: 0.774 | accuracy: 0.834 | valid_loss: 0.408 | train_loss: 0.325 | best_score: 0.806\n","output_type":"stream"},{"name":"stderr","text":" 95%|█████████▌| 1900/2000 [37:15<23:57, 14.38s/it]","output_type":"stream"},{"name":"stdout","text":"01900 steps | f1: 0.806 | precision: 0.790 | recall: 0.822 | accuracy: 0.834 | valid_loss: 0.436 | train_loss: 0.349 | best_score: 0.806\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2000/2000 [39:11<00:00,  1.18s/it]","output_type":"stream"},{"name":"stdout","text":"02000 steps | f1: 0.799 | precision: 0.828 | recall: 0.771 | accuracy: 0.838 | valid_loss: 0.398 | train_loss: 0.347 | best_score: 0.806\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Evaluate","metadata":{}},{"cell_type":"code","source":"model.load_state_dict(torch.load('/kaggle/input/model/model_sd.pt'))","metadata":{"execution":{"iopub.status.busy":"2023-04-26T18:28:53.961827Z","iopub.execute_input":"2023-04-26T18:28:53.962226Z","iopub.status.idle":"2023-04-26T18:29:02.246423Z","shell.execute_reply.started":"2023-04-26T18:28:53.962191Z","shell.execute_reply":"2023-04-26T18:29:02.245269Z"},"trusted":true},"execution_count":31,"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"code","source":"def evaluate(model, valid_dataloader, metrics=[('f1', f1_score),('precision', precision_score), ('recall', recall_score),('accuracy', accuracy_score)]):\n    model.eval()\n\n    y_trues = []\n    y_hats = []\n\n    with torch.no_grad():\n        for batch in valid_dataloader:\n\n            (ids, mask), y_true = batch\n            y_hat = torch.softmax(model.forward(input_ids=ids, attention_mask=mask),dim=-1)\n\n            for i in range(y_true.shape[0]):\n                y_trues.append(int(y_true[i]))\n                y_hats.append(1 if y_hat[i][0] < y_hat[i][1] else 0)\n\n    scores = []\n\n    for metric in metrics:\n        scores.append((metric[0], metric[1](y_trues, y_hats)))        \n\n    return scores","metadata":{"execution":{"iopub.status.busy":"2023-04-26T18:29:09.113491Z","iopub.execute_input":"2023-04-26T18:29:09.113858Z","iopub.status.idle":"2023-04-26T18:29:09.122504Z","shell.execute_reply.started":"2023-04-26T18:29:09.113825Z","shell.execute_reply":"2023-04-26T18:29:09.121251Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"scores = evaluate(model, valid_dataloader)\nprint(scores)","metadata":{"execution":{"iopub.status.busy":"2023-04-26T18:29:16.199073Z","iopub.execute_input":"2023-04-26T18:29:16.199771Z","iopub.status.idle":"2023-04-26T18:30:00.644287Z","shell.execute_reply.started":"2023-04-26T18:29:16.199734Z","shell.execute_reply":"2023-04-26T18:30:00.643114Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"[('f1', 0.8713178294573644), ('precision', 0.8740279937791602), ('recall', 0.8686244204018547), ('accuracy', 0.8891115564462257)]\n","output_type":"stream"}]},{"cell_type":"code","source":"predictions_df = pd.DataFrame()\nfor i, (ids, mask) in tqdm(dataset_test):\n    pred = model(input_ids=ids[None], attention_mask=mask[None])[0]\n    y_hat = 1 if pred[0] < pred[1] else 0\n    r = [int(i), y_hat]\n    predictions_df = pd.concat([predictions_df, pd.DataFrame(np.array(r)[None,:], columns=['id', 'target'])])","metadata":{"execution":{"iopub.status.busy":"2023-04-26T18:30:28.428480Z","iopub.execute_input":"2023-04-26T18:30:28.428875Z","iopub.status.idle":"2023-04-26T18:32:21.879202Z","shell.execute_reply.started":"2023-04-26T18:30:28.428838Z","shell.execute_reply":"2023-04-26T18:32:21.877946Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stderr","text":"100%|██████████| 3263/3263 [01:53<00:00, 28.76it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"predictions_df","metadata":{"execution":{"iopub.status.busy":"2023-04-26T18:32:36.788827Z","iopub.execute_input":"2023-04-26T18:32:36.789399Z","iopub.status.idle":"2023-04-26T18:32:36.800978Z","shell.execute_reply.started":"2023-04-26T18:32:36.789352Z","shell.execute_reply":"2023-04-26T18:32:36.799900Z"},"trusted":true},"execution_count":35,"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"       id  target\n0       0       1\n0       2       1\n0       3       1\n0       9       1\n0      11       1\n..    ...     ...\n0   10861       0\n0   10865       1\n0   10868       1\n0   10874       1\n0   10875       1\n\n[3263 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>3</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>9</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>11</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>10861</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>10865</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>10868</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>10874</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>10875</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>3263 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"predictions_df.to_csv('submission_final.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-04-26T18:39:44.046103Z","iopub.execute_input":"2023-04-26T18:39:44.047013Z","iopub.status.idle":"2023-04-26T18:39:44.057659Z","shell.execute_reply.started":"2023-04-26T18:39:44.046961Z","shell.execute_reply":"2023-04-26T18:39:44.056538Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"# def finetune_epoch(clf: FinetuneClassifier,\n#                    train_dataset,\n#                    batch_size=16,\n#                    lr=1e-4,\n#                    loss_lambda = 0.25,\n#                    device=\"cuda:0\"):\n#     clf.train()\n#     loader = DataLoader(train_dataset, batch_size, drop_last=True)\n#       # train_features, train_labels = next(iter(loader))\n#       # print(f\"Feature batch shape: {train_features.size()}\")\n#       # print(f\"Labels batch shape: {train_labels.size()}\")\n\n#     params_to_update = []\n#     for name,param in clf.named_parameters():\n#         if param.requires_grad == True:\n#             params_to_update.append(param)\n\n#     optimizer = torch.optim.Adam(params_to_update, lr=lr, eps=1e-08)\n#     criterion = nn.CrossEntropyLoss(ignore_index=-1000).to(device)\n\n#     idx = 0\n\n#     # Train loop.\n#     optimizer.zero_grad()\n#     logits_dep = clf(tokens)\n    \n#     # print(logits_dep.size())\n\n#     total_loss_dep = 0\n\n#     for i in range(batch_size):\n#         id = batch['id'][i].to(device)\n\n#         dep_label = train_data_dict['dep_label'][id]\n#         rel_pos = train_data_dict['rel_pos'][id]\n#         dep_label_idx = dep_to_index(dep_label)\n#         rel_pos_idx = rel_pos_to_index(rel_pos)\n\n#         UD_tokenized = convert_token_to_UD(batch['text'][i], len(dep_label_idx))\n\n#         target_len = len(logits_dep[i])\n#         dep_idx = [-1000]*target_len\n#         rel_idx = [-1000]*target_len\n\n#         k=0\n#         for j in UD_tokenized:\n#         # if j in UD_tokenized:\n#         # print(j)\n#         # print(len(dep_label_idx))\n#         # print(len(dep_idx))\n#         dep_idx[j] = dep_label_idx[k]\n#         rel_idx[j] = rel_pos_idx[k]\n#         k+=1\n\n#         # print(rel_pos)\n#         # print(rel_pos_idx)\n#         # print(rel_idx)\n\n#         torch_dep_idx = torch.tensor(dep_idx).to(device)\n#         torch_rel_idx = torch.tensor(rel_idx).to(device)\n\n\n#         # print(torch_dep_idx.size())\n#         # print(torch_rel_idx.size())\n#         # print(logits_dep[i].size())\n#         # print(logits_rel[i])\n#         # # print(logits_dep[i][focus, :])\n#         # # print(dep_label_idx.size())\n\n#         loss_dep = criterion(logits_dep[i], torch_dep_idx)\n#         total_loss_dep += loss_dep\n#         loss_rel = criterion(logits_rel[i], torch_rel_idx)\n#         total_loss_rel += loss_rel\n\n#     loss = (loss_lambda*total_loss_rel+(1-loss_lambda) * total_loss_dep)\n#     loss.backward()\n#     # Have gradients at this point.\n#     nn.utils.clip_grad_norm_(clf.parameters(), max_norm=1.0, norm_type=2)\n#     optimizer.step()\n\n#     if idx % 50 == 0:\n#       print(loss.item())\n#     idx += 1","metadata":{"execution":{"iopub.status.busy":"2023-04-26T02:00:47.925829Z","iopub.status.idle":"2023-04-26T02:00:47.926532Z","shell.execute_reply.started":"2023-04-26T02:00:47.926234Z","shell.execute_reply":"2023-04-26T02:00:47.926261Z"},"trusted":true},"execution_count":null,"outputs":[]}]}